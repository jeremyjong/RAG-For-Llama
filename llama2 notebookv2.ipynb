{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jerem\\.conda\\envs\\llm\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import transformer classes for generaiton\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "# Import torch for datatype attributes \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variable to hold llama2 weights naming \n",
    "name = \"elyza/ELYZA-japanese-Llama-2-7b-instruct\"\n",
    "# Set auth token variable from hugging face \n",
    "auth_token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jerem\\.conda\\envs\\llm\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:631: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(name, \n",
    "    cache_dir='./model/', use_auth_token=auth_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jerem\\.conda\\envs\\llm\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:460: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "\"\"\"model = AutoModelForCausalLM.from_pretrained(name, \n",
    "    cache_dir='./model/', use_auth_token=auth_token, torch_dtype=torch.float16, \n",
    "    rope_scaling={\"type\": \"dynamic\", \"factor\": 2}, load_in_8bit=False) \"\"\"\n",
    "# Create model\n",
    "model = AutoModelForCausalLM.from_pretrained(name, \n",
    "    cache_dir='./model/', use_auth_token=auth_token, torch_dtype=\"auto\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "# パイプラインの準備\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256\n",
    ")\n",
    "\n",
    "# LLMの準備\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello [/INST]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from llama_index import LangchainEmbedding\n",
    "from typing import Any, List\n",
    "\n",
    "# 埋め込みクラスにqueryを付加\n",
    "class HuggingFaceQueryEmbeddings(HuggingFaceEmbeddings):\n",
    "    def __init__(self, **kwargs: Any):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return super().embed_documents([\"query: \" + text for text in texts])\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return super().embed_query(\"query: \" + text)\n",
    "\n",
    "# 埋め込みモデルの準備\n",
    "embed_model = LangchainEmbedding(\n",
    "    HuggingFaceQueryEmbeddings(model_name=\"intfloat/multilingual-e5-large\")\n",
    "\n",
    "\"\"\"embed_model=LangchainEmbedding(\n",
    "    HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    ")   \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in stuff to change service context\n",
    "from llama_index import set_global_service_context\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.text_splitter import SentenceSplitter\n",
    "from llama_index.node_parser import SimpleNodeParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ノードパーサーの準備\n",
    "text_splitter = SentenceSplitter(\n",
    "    chunk_size=500,\n",
    "    paragraph_separator=\"\\n\\n\",\n",
    "    tokenizer=tokenizer.encode\n",
    ")\n",
    "node_parser = SimpleNodeParser.from_defaults(text_splitter=text_splitter)\n",
    "\n",
    "# サービスコンテキストの準備\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=embed_model,\n",
    "    node_parser=node_parser,\n",
    ")\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import deps to load documents \n",
    "from llama_index import VectorStoreIndex, download_loader\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Download PDF Loader \n",
    "PyMuPDFReader = download_loader(\"PyMuPDFReader\")\n",
    "# Create PDF Loader\n",
    "loader = PyMuPDFReader()\n",
    "# Load documents \n",
    "documents = loader.load(file_path=Path('./data/2024_1q_summary_jp.pdf'), metadata=True)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "# ドキュメントの読み込み\n",
    "documents = SimpleDirectoryReader(\n",
    "    \"./data\"\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "# インデックスの作成\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    service_context=service_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.prompts.prompts import QuestionAnswerPrompt\n",
    "\n",
    "# QAテンプレートの準備\n",
    "qa_template = QuestionAnswerPrompt(\"\"\"<s>[INST] <<SYS>>\n",
    "質問に100文字以内で答えだけを回答してください。\n",
    "<</SYS>>\n",
    "{query_str}\n",
    "\n",
    "{context_str} [/INST]\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# クエリエンジンの作成\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=3,\n",
    "    text_qa_template=qa_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out a query in natural\n",
    "response = query_engine.query(\"2024年の配当金を教えてください。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
